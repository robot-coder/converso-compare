# README.md

# Web-Based Chat Assistant with Multi-Model Comparison

This project implements a web-based chat assistant that provides a continuous, themed conversation experience. Users can upload text and image files, and compare responses generated by two different large language models (LLMs). The application is deployed on Render.com for easy access and scalability.

## Features

- **Themed, continuous conversations** with context preservation
- **File uploads** (text and images) for enhanced interactions
- **Comparison of responses** from two different LLMs
- **Responsive web UI** built with HTML, CSS, and JavaScript
- **Backend API** powered by FastAPI
- **Deployment** on Render.com

## Technologies Used

- Python 3.11+
- FastAPI
- Uvicorn
- LiteLLM (or similar lightweight LLM interface)
- Starlette (for static files)
- JavaScript, HTML, CSS (for front-end)
- httpx (for HTTP requests)

## Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/chat-assistant.git
cd chat-assistant
```

### 2. Install dependencies

Create a virtual environment and install required packages:

```bash
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Run the application locally

```bash
uvicorn main:app --reload
```

Open your browser and navigate to `http://127.0.0.1:8000` to access the chat interface.

### 4. Deploy on Render.com

- Push your code to a GitHub repository.
- Create a new Web Service on Render.
- Connect your repository.
- Set the start command to:

```bash
uvicorn main:app --host 0.0.0.0 --port 10000
```

- Ensure environment variables and static files are configured as needed.

## File Structure

- `main.py` — Main application code
- `requirements.txt` — Dependencies
- `static/` — Static files (CSS, JS, images)
- `templates/` — HTML templates (if used)

## License

This project is licensed under the MIT License.

---

## Example `main.py`

```python
from fastapi import FastAPI, Request, UploadFile, File, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from typing import List, Optional
import uvicorn
import httpx
import os

# Placeholder for LLM interface
class LiteLLM:
    def __init__(self, model_name: str):
        self.model_name = model_name

    def generate_response(self, prompt: str) -> str:
        # Implement actual LLM call here
        return f"Response from {self.model_name} for prompt: {prompt}"

app = FastAPI()

# Mount static files for frontend assets
app.mount("/static", StaticFiles(directory="static"), name="static")

# Initialize two different LLMs for comparison
llm1 = LiteLLM("Model_A")
llm2 = LiteLLM("Model_B")

# Store conversation context
conversation_history: List[str] = []

@app.get("/", response_class=HTMLResponse)
async def get_home():
    with open("templates/index.html", "r") as file:
        html_content = file.read()
    return HTMLResponse(content=html_content)

@app.post("/chat/")
async def chat_endpoint(request: Request):
    data = await request.json()
    user_message: str = data.get("message", "")
    theme: Optional[str] = data.get("theme", "default")
    # Append user message to history
    conversation_history.append(user_message)

    prompt = "\n".join(conversation_history)
    try:
        response1 = llm1.generate_response(prompt)
        response2 = llm2.generate_response(prompt)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

    # Optionally, store or process responses
    return JSONResponse(content={
        "response_model_a": response1,
        "response_model_b": response2,
        "history": conversation_history
    })

@app.post("/upload/")
async def upload_files(files: List[UploadFile] = File(...)):
    responses = {}
    for file in files:
        filename = file.filename
        try:
            content = await file.read()
            # Save or process the file as needed
            responses[filename] = "File received successfully"
        except Exception as e:
            responses[filename] = f"Error: {str(e)}"
    return JSONResponse(content=responses)

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
```

## Example `requirements.txt`

```
fastapi
uvicorn
liteLLM
httpx
starlette
```

---

**Note:** Replace placeholder code with actual LLM API calls and implement the front-end UI in `static/` and `templates/` directories as needed.